{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "import lightning as L\n",
    "from loguru import logger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRDataModule(L.LightningDataModule):\n",
    "    \"\"\"Data module for the Mon Reader model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path | str,\n",
    "        transform: Optional[Callable] = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): The root directory of the dataset.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        \"\"\"Setup.\n",
    "        Args:\n",
    "          stage: Optional[str], default=None. The stage of the setup.\n",
    "        \"\"\"\n",
    "        # Loading the training dataset. We need to split it into a training and validation part\n",
    "        train_set = datasets.ImageFolder(\n",
    "            root=f\"{self.hparams['data_dir']}/training\",\n",
    "            transform=self.hparams.transform,\n",
    "        )\n",
    "        self.train_set, self.val_set = data.random_split(train_set, [0.75, 0.25])\n",
    "        # Loading the test set\n",
    "        self.test_set = datasets.ImageFolder(\n",
    "            root=f\"{self.hparams['data_dir']}/testing\", transform=self.hparams.transform\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Return the training dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The training dataloader.\n",
    "        \"\"\"\n",
    "        return data.DataLoader(\n",
    "            self.train_set, batch_size=self.hparams[\"batch_size\"], shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Return the validation dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams[\"batch_size\"], shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Return the test dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The test dataloader.\n",
    "        \"\"\"\n",
    "        return data.DataLoader(\n",
    "            self.test_set, batch_size=self.hparams[\"batch_size\"], shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "class MRNet(L.LightningModule):\n",
    "    def __init__(self, model_params, optimizer_hparams):\n",
    "        \"\"\"CIFARModule.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams: Hyperparameters for the model, as dictionary.\n",
    "            optimizer_hparams: Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = create_model(**model_params)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        # Metrics\n",
    "        self.metrics = torchmetrics.MetricCollection(\n",
    "            {\n",
    "                \"f1\": torchmetrics.F1Score(\n",
    "                    num_classes=model_params[\"num_classes\"], task=\"binary\"\n",
    "                ),\n",
    "                \"accuracy\": torchmetrics.Accuracy(\n",
    "                    num_classes=model_params[\"num_classes\"], task=\"binary\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # Forward function\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[50, 100], gamma=0.1\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self._compute_metrics(labels, preds, \"train\")\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        self._compute_metrics(labels, preds, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
    "        self._compute_metrics(labels, preds, \"test\")\n",
    "\n",
    "    def _compute_metrics(self, labels, preds, stage):\n",
    "        metric_results = self.metrics(preds.argmax(dim=-1), labels)\n",
    "        for name, result in metric_results.items():\n",
    "            self.log(f\"{stage}_{name}\", result, on_epoch=True)\n",
    "\n",
    "\n",
    "# Models\n",
    "\n",
    "\n",
    "class CustomMRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMRNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(32 * 56 * 56, 128), nn.ReLU(), nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_model(model_name: str, num_classes: int, **kwargs):\n",
    "    # Choose a pre-trained model from torchvision\n",
    "    if model_name == \"custom\":\n",
    "        model = CustomMRNet()\n",
    "    else:\n",
    "        model = models.get_model(\n",
    "            model_name, weights=None, num_classes=num_classes, **kwargs\n",
    "        )\n",
    "        model.train()\n",
    "    # print(model)\n",
    "    # # Replace the last fully connected layer with a new one for binary classification\n",
    "    # num_features = model.fc.in_features\n",
    "    # model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    data_params,\n",
    "    model_params,\n",
    "    optimizer_hparams,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "    num_epochs: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Train model.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional): If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    L.seed_everything(42)\n",
    "\n",
    "    data = MRDataModule(**data_params)\n",
    "    data.setup()\n",
    "    print(f\"Train --->{len(data.train_set)}\")\n",
    "    print(f\"Val --->{len(data.val_set)}\")\n",
    "    print(f\"Test --->{len(data.test_set)}\")\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=os.path.join(\n",
    "            checkpoint_path, model_params[\"model_name\"]\n",
    "        ),  # Where to save models\n",
    "        # We run on a single GPU (if possible)\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        # How many epochs to train for if no patience is set\n",
    "        max_epochs=num_epochs,\n",
    "        # limit_train_batches= 2, # FOR DEBUG\n",
    "        # limit_val_batches = 2, # FOR DEBUG\n",
    "        # limit_test_batches= 2, #FOR DEBUG\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                save_weights_only=True, mode=\"max\", monitor=\"val_accuracy\"\n",
    "            ),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],  # Log learning rate every epoch\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = (\n",
    "        None  # Optional logging argument that we don't need\n",
    "    )\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    model_name = model_params[\"model_name\"]\n",
    "    pretrained_filename = os.path.join(checkpoint_path, f\"{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = MRNet.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        L.seed_everything(42)  # To be reproducible\n",
    "        model = MRNet(model_params, optimizer_hparams)\n",
    "        trainer.fit(model, data)\n",
    "        model = MRNet.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )  # Load best checkpoint after training\n",
    "\n",
    "    # Test best model\n",
    "    test_result = trainer.test(model, data, verbose=False)\n",
    "    return {\"model\": model_name, **test_result[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\"data_dir\": \"../images/images\", \"batch_size\": 16}\n",
    "model_params = {\"num_classes\": 2}\n",
    "checkpoint_path = \"../.checkpoints/\"\n",
    "optimizer_hparams = {\"lr\": 1e-3, \"weight_decay\": 1e-4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | CustomMRNet      | 12.9 M\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "2 | metrics     | MetricCollection | 0     \n",
      "-------------------------------------------------\n",
      "12.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.9 M    Total params\n",
      "51.402    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train --->1794\n",
      "Val --->598\n",
      "Test --->597\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogia/p99pe5XYBfCuAWBK/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogia/p99pe5XYBfCuAWBK/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 113/113 [01:51<00:00,  1.01it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 113/113 [01:51<00:00,  1.01it/s, v_num=1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogia/p99pe5XYBfCuAWBK/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 38/38 [00:18<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline = train_model(\n",
    "    data_params,\n",
    "    {\"model_name\": \"custom\", **model_params},\n",
    "    optimizer_hparams,\n",
    "    checkpoint_path,\n",
    "    num_epochs = 100\n",
    ") # Only running 2 batches for 1 one epoch - just for Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train --->1794\n",
      "Val --->598\n",
      "Test --->597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ../.checkpoints/alexnet/lightning_logs\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | AlexNet          | 57.0 M\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "2 | metrics     | MetricCollection | 0     \n",
      "-------------------------------------------------\n",
      "57.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "57.0 M    Total params\n",
      "228.048   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:08<00:00,  0.25it/s, v_num=0]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:08<00:00,  0.23it/s, v_num=0]\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train --->1794\n",
      "Val --->598\n",
      "Test --->597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: ../.checkpoints/convnext_base/lightning_logs\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | ConvNeXt         | 87.6 M\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "2 | metrics     | MetricCollection | 0     \n",
      "-------------------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.274   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pretrained_models = [\n",
    "    train_model(\n",
    "        data_params,\n",
    "        {\"model_name\": model, **model_params},\n",
    "        optimizer_hparams,\n",
    "        checkpoint_path,\n",
    "    )\n",
    "    for model in models.list_models()\n",
    "] # I was not able to run them on my computer because too heavy but the code is functionnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model  test_accuracy   test_f1\n",
      "0  custom       0.998325  0.517588\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "print(DataFrame([baseline])) #print(DataFrame([baseline] + pretrained_models))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
